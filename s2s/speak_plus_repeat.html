<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Speech-to-Text with Voice Selection</title>
<style>
  body { font-family: sans-serif; text-align: center; margin-top: 50px; background:black; color:white }
  #indicator {
    display: inline-block;
    width: 30px;
    height: 30px;
    margin-left: 10px;
    border-radius: 50%;
    background-color: gray;
    vertical-align: middle;
    transition: width 0.1s, height 0.1s;
  }
  canvas { display: block; margin: 20px auto; background: #222; border-radius: 8px; }
  select { font-size: 16px; margin-top: 10px; }
</style>
</head>
<body>
        <img src="speak2.png"/>

<h2>Speak:</h2>
<p id="output">...</p>
<p>Listening indicator: <span id="indicator"></span></p>
<select id="voiceSelect"></select>
<canvas id="waveform" width="600" height="100"></canvas>

<script>
const output = document.getElementById('output');
const indicator = document.getElementById('indicator');
const voiceSelect = document.getElementById('voiceSelect');
const canvas = document.getElementById('waveform');
const ctx = canvas.getContext('2d');

const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
if (!SpeechRecognition) alert("Speech recognition not supported in this browser.");

let recognition, silenceTimeout;
var currentSpeech = ""; // Global variable for current speech
let audioContext, analyser, dataArray, source;

// Setup audio for waveform visualization
async function setupAudio() {
    audioContext = new (window.AudioContext || window.webkitAudioContext)();
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    source = audioContext.createMediaStreamSource(stream);

    analyser = audioContext.createAnalyser();
    analyser.fftSize = 256;
    source.connect(analyser);
    dataArray = new Uint8Array(analyser.frequencyBinCount);

    drawWaveform();
}

// Draw simple waveform
function drawWaveform() {
    requestAnimationFrame(drawWaveform);
    if (!analyser) return;

    analyser.getByteTimeDomainData(dataArray);
    ctx.fillStyle = '#222';
    ctx.fillRect(0, 0, canvas.width, canvas.height);

    ctx.lineWidth = 2;
    ctx.strokeStyle = "red";
    ctx.beginPath();

    const sliceWidth = canvas.width / dataArray.length;
    let x = 0;
    let sum = 0;

    for (let i = 0; i < dataArray.length; i++) {
        const v = dataArray[i] / 128.0 - 1.0;
        sum += Math.abs(v);
        const y = (v + 1) * canvas.height / 2;
        if (i === 0) ctx.moveTo(x, y);
        else ctx.lineTo(x, y);
        x += sliceWidth;
    }
    ctx.stroke();

    const avg = sum / dataArray.length;
    const size = 20 + avg * 50;
    indicator.style.width = `${size}px`;
    indicator.style.height = `${size}px`;
}

// Populate voice dropdown
function populateVoices() {
    const voices = speechSynthesis.getVoices();
    voiceSelect.innerHTML = '';
    voices.forEach((v, i) => {
        const option = document.createElement('option');
        option.value = i;
        option.textContent = `${v.name} (${v.lang})`;
        voiceSelect.appendChild(option);
    });
}
speechSynthesis.onvoiceschanged = populateVoices;

// Start speech recognition
function startListening() {
    recognition = new SpeechRecognition();
    recognition.lang = 'en-US';
    recognition.interimResults = true;
    recognition.continuous = true;

    indicator.style.backgroundColor = 'green';

    recognition.onresult = (event) => {
        // Only keep current speech in global variable
        currentSpeech = Array.from(event.results)
            .map(r => r[0].transcript)
            .join('');
        output.textContent = currentSpeech;

        if (silenceTimeout) clearTimeout(silenceTimeout);
        silenceTimeout = setTimeout(() => speakBack(currentSpeech), 1000);
    };

    recognition.onerror = (event) => {
        console.error("Error:", event.error);
        stopListening();
        setTimeout(startListening, 500);
    };

    recognition.onend = () => {
        indicator.style.backgroundColor = 'gray';
        setTimeout(startListening, 500);
    };

    recognition.start();
}

// Speak back and reset global variable
function speakBack(text) {
    if (!text) return;
    const utterance = new SpeechSynthesisUtterance(text);

    const voices = speechSynthesis.getVoices();
    const selectedIndex = voiceSelect.selectedIndex;
    if (voices[selectedIndex]) utterance.voice = voices[selectedIndex];

    utterance.onstart = () => indicator.style.backgroundColor = 'gray';
    utterance.onend = () => {
        currentSpeech = "";      // Reset global variable
        output.textContent = "..."; // Clear display
    };

    speechSynthesis.speak(utterance);
}

// Stop recognition
function stopListening() {
    if (recognition) {
        recognition.onresult = null;
        recognition.onerror = null;
        recognition.onend = null;
        recognition.stop();
        recognition = null;
    }
    if (silenceTimeout) clearTimeout(silenceTimeout);
    indicator.style.backgroundColor = 'gray';
}

// Initialize
setupAudio();
populateVoices();
startListening();
</script>
</body>
</html>
